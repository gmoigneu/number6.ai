---
title: "Stop calling everything 'AI-powered'"
subtitle: "How to tell real AI capability from marketing rebrand"
date: 2026-02-07
author: g
category: honest-take
tags: ["honest take", "industry"]
excerpt: "The 'AI-powered' label has become meaningless. Here's how to spot real AI capability vs. buzzword abuse when you're buying tools."
featured_image: "/images/blog/22-stop-calling-everything-ai-powered-art.png"
featured_image_alt: "A product label covered in overlapping marketing buzzword stickers"
featured: false
draft: false
---

Somewhere in the past two years, "AI-powered" became the "organic" of the software world. Slap it on the label and suddenly the product is worth twice as much.

Your email client is now AI-powered. Your project management tool is AI-powered. The accounting software you've used since 2018 released an update, added a button that generates a one-sentence summary, and started calling itself AI-powered. The feature that used to be called "smart filters" or "auto-suggest" is now "AI-driven intelligence." Same feature. New label. Higher price tier.

This is AI washing, and it's making it genuinely harder for business owners to figure out which tools actually use AI in a meaningful way and which ones just discovered that the letters A and I look good in a press release.

## What AI washing actually looks like

The term comes from "greenwashing," where companies slap environmental claims on products without doing much to earn them. AI washing works the same way. A company rebrands existing features, adds the word "AI" to the marketing copy, and hopes you don't ask too many questions.

Here are the patterns we keep seeing.

The most common one: a tool that's been doing rule-based automation for years suddenly becomes "AI-powered automation." If your email filter sorts messages by keywords into folders, that's a rule. It's not AI. It was a useful feature before the rebrand and it's the same useful feature after. But now it costs more.

Then there's the "AI-assisted" label on anything that touches a large language model, even if the integration is paper-thin. A project management tool that lets you click a button to "generate a task description with AI" isn't an AI-powered project management platform. It's a project management platform that added a ChatGPT API call to one screen. The difference matters when you're evaluating whether to pay $30/user/month versus $15/user/month.

The third pattern is what we'd call the AI adjacency play. The tool doesn't do anything with AI itself, but it connects to tools that do, and the marketing implies the AI capability is built in. "AI-ready integrations" sounds impressive until you realize it means "we have an API."

## Why this matters when you're buying

If you're running a business with 20 or 50 or 100 people, and you're trying to figure out where AI can actually help, AI washing is actively working against you.

It wastes your evaluation time. You schedule a demo, sit through a pitch, realize the "AI" is a search bar with better autocomplete, and you've burned an afternoon. Do that with six vendors and you've lost a week just sorting real from fake.

It also inflates your costs. When every tool adds $10-20/user/month for an "AI tier" that doesn't produce meaningfully different results, you're paying a marketing premium. Across your whole software stack, that number gets uncomfortable fast.

But the cost we care about most is harder to measure: **trust.** When your team tries three "AI-powered" tools and finds them underwhelming, they stop believing AI can help at all. The next time you bring a tool to the table that genuinely saves them hours of work, they're already skeptical. You've spent your credibility budget on tools that didn't deserve the label.

## The one question that cuts through the noise

I keep coming back to a single test, and it's almost embarrassingly simple.

**"What does this tool do that couldn't be done before AI?"**

Not "what does it do better" or "what does it do faster." What does it do that was previously impossible, or at least wildly impractical?

Take contract review. If a tool can summarize 200 pages and flag the clauses that deviate from your standard terms in under a minute, that's real. A human could do that, but it would take hours. The AI turns something impractical into something routine.

Now compare that to email sorting. "It categorizes your messages automatically." Sure, but email filters have been doing that since the 1990s. Maybe the AI version is slightly smarter about edge cases, but is it $15/user/month smarter? Probably not.

Drafting customer responses based on your past replies and your company's tone? That's real, too. It's generating new content from patterns in your existing data. A template library can't do that.

"Intelligent analytics" is where things get murkier. Ask what "intelligent" means. Specifically. If the vendor can't point to an insight the tool surfaces that a standard dashboard wouldn't, the word "intelligent" is doing a lot of work for very little substance.

## Real AI capability vs. the marketing version

A few categories where the gap between "actual AI" and "AI on the label" is especially wide.

In document processing, real AI reads unstructured documents and extracts data points without needing templates or fixed formats. It handles variation in how things are written. A contract with unusual formatting? Still works. What gets called "smart document management" is often just template-based field mapping. You set it up manually, it auto-fills based on those rules. That's automation, and it's useful, but it's not AI.

Customer service is where the labeling gets most egregious. A tool that understands natural language, searches your knowledge base, and generates answers to questions phrased in ways nobody anticipated? That's AI. A chatbot with a decision tree where "if customer says X, respond with Y, and if they say anything else, show a contact button"? That's a flowchart with a chat interface.

For content creation, the line is actually pretty clear. If the tool generates original text that adapts to your input and produces different output each time, that's AI doing something new. "Dear [FirstName], thank you for your [ProductType] purchase" is mail merge. We've had mail merge since the 1980s.

Analytics is the muddiest category. Real AI finds patterns you didn't ask it to look for. It surfaces anomalies, predicts trends, clusters your customers in ways you hadn't considered. Pre-built dashboards with filters are business intelligence. We had BI tools in the early 2000s. Rebranding them doesn't change what they are.

## How vendors get away with it

Part of the problem is that there's no standard definition of "AI-powered." No certification. No regulatory bar to clear (at least not yet, in most markets). A company can call a feature AI-powered if it uses a single API call to a language model, or if it uses a basic statistical model from 2015, or if it does nothing with AI at all but the marketing team thought it sounded good.

The other part is that most buyers don't know enough about AI to push back. That's not an insult. Why would you? You're running a business, not studying machine learning. The vendor knows more about AI than you do, and they know that you know that. So when they say "AI-powered," you take it at face value.

This is exactly why the "what couldn't be done before" question is so useful. You don't need to understand how the AI works. You just need to understand what it does. If the vendor can't clearly explain a capability that AI makes possible (not just slightly faster), the AI label probably isn't earning its keep.

## What to do about it

When you're evaluating tools, here's what we recommend.

Ignore the "AI" label entirely in your initial screen. Look at what the tool does, not what it calls itself. Does it solve a real problem you have? Does it do something specific that would save your team time or produce better results? Start there.

When a vendor brings up AI, ask for specifics. Which features use AI? What model or approach do they use? What happens to the "AI features" if the underlying model changes or becomes unavailable? You don't need technical expertise to ask these questions. The answers will tell you a lot.

Compare pricing tiers carefully. If the "AI tier" costs significantly more, look at exactly which features you're paying extra for. Try the non-AI tier first if possible. Sometimes the base product does everything you need.

Talk to actual users, not just the sales team. Ask other businesses in your space whether the AI features made a real difference to their work. "We use the AI summarization every day, it saves us three hours a week" is a very different answer from "I think we have that turned on somewhere."

## The catch (because there's always a catch)

Being skeptical about AI claims can tip into being dismissive of AI altogether, and that's its own problem. Some tools genuinely deserve the label. Language models that draft, summarize, and translate are doing things that weren't practical two years ago. Computer vision tools that read handwritten forms are real AI doing real work. Voice transcription that's actually good enough to replace note-taking is a genuine advance.

**Make vendors prove the label.** If they can show you a capability that's new, useful, and actually dependent on AI to function, that's worth paying for. If they can't, you're paying for a sticker.

I think we're about twelve to eighteen months from the market self-correcting on this. Buyers are getting smarter. Vendors who over-promise are getting called out. The tools that deliver real results will win, and the ones riding the label will quietly drop it.

Until then, ask the question. "What does this do that couldn't be done before?" The vendors who have a good answer will be happy you asked. The ones who don't will change the subject.
