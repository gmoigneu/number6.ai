---
title: "How to write prompts that get useful results"
subtitle: "Think less like a programmer and more like someone briefing a freelancer"
date: 2025-10-03
author: g
category: ai-for-humans
tags: ["prompt engineering", "productivity"]
excerpt: "A non-technical guide to writing better AI prompts, with real business examples and before-and-after comparisons that show the difference."
featured_image: "/images/blog/03-how-to-write-prompts-that-get-useful-results-art.png"
featured_image_alt: "A person writing notes on paper, planning out instructions"
featured: false
draft: false
---

You've signed up for ChatGPT or Claude. You've typed in a question. The answer came back and it was... fine. Kind of generic. Not quite what you needed. So you tried again, got something slightly different but still not right, and eventually gave up and wrote the thing yourself.

This is the most common experience people have with AI tools, and it's almost always a prompting problem, not a tool problem.

The good news: you don't need to learn "prompt engineering" in any technical sense. You need to learn how to write a good brief. If you've ever hired a freelancer, you already know most of what matters.

## The freelancer analogy

Think about the last time you hired someone to do a task for you. Maybe a graphic designer, a copywriter, or a virtual assistant. If you said "make me a logo," you'd get something. It probably wouldn't be what you wanted. But if you said "make me a logo for a consulting firm that works with small businesses, the tone should feel professional but approachable, avoid corporate blue, and here are three logos I like as references," you'd get something much closer.

AI tools work the same way. **The quality of what you get out is directly proportional to the quality of the brief you put in.** A vague prompt gets a vague answer. A specific prompt gets a specific, useful answer.

The difference between people who find AI tools useful and people who don't usually comes down to this one skill.

## Be specific about what you want

Compare these two prompts:

*Vague:* "Write me an email to a client."

*Specific:* "Write a follow-up email to a client named Sarah who attended our product demo last Tuesday. She seemed interested but had concerns about pricing. The tone should be warm and helpful, not pushy. Keep it under 150 words. Mention our flexible payment plans."

The vague version could produce anything. The specific version gives the AI enough context to produce something you can actually send (after a quick review and personal touch).

What makes the specific one better? It answers the questions any competent freelancer would ask before starting work: Who is the audience? What's the tone? How long should it be? Is there anything it should definitely include or avoid? What format do you want?

You don't need to answer all of these every time. But the more of them you cover, the better the output gets.

## Give it context about your situation

AI tools don't know anything about your business unless you tell them. This is the part people forget most often.

Here's a prompt without context: "Help me respond to this customer complaint."

And the same prompt with context: "I run a 30-person landscaping company. A customer emailed complaining that our crew left debris in their yard after a job. This is the third complaint from this customer in six months, but they're one of our biggest accounts. I need a response that takes their complaint seriously without being overly apologetic, because our crew lead says the debris was from a tree that fell after we left. Keep the tone professional and firm but respectful."

Those two prompts produce completely different outputs. Without context, you get a generic apology template. With it, you get something that actually fits your situation.

A practical habit worth building: **start your prompts by telling the AI who you are and what your business does.** Just one or two sentences. "I'm the operations manager at a 50-person accounting firm" gives the AI enough to calibrate its tone, vocabulary, and assumptions.

## Ask for a specific format

This is a small thing that makes a surprisingly big difference. If you need bullet points, say so. If you want a table or a document broken into sections with headers, tell it.

Without any format guidance, "Summarize this report for my team" will get you a wall of text. But "Summarize this report in 5-7 bullet points that I can paste into a Slack message. Focus on action items and decisions, skip the background context. Use plain language, no jargon" gets you something immediately useful.

You can get very specific about structure:

"Give me a comparison table with columns for tool name, monthly cost, best use case, and biggest limitation."

"Write this as a numbered list of steps someone with no technical background could follow."

"Format this as a one-page brief with an executive summary at the top, three key findings in the middle, and a recommended next step at the bottom."

The AI will match whatever structure you describe. Most people never think to ask, so they get whatever default format the tool decides on.

## Iterate instead of starting over

Here's the thing most people miss: you're not limited to one prompt. A conversation with an AI tool is exactly that. A conversation. You can refine and build on what it gives you.

Your first prompt gets you maybe 60% of the way there. The next two or three prompts get you to 90%. That last 10% is your own editing. This is how people who get real value from AI tools actually work. They don't expect the first output to be perfect. They expect to go back and forth a few times.

Some follow-ups that work well:

"That's close but too formal. Make it more conversational."

"Good structure, but section 2 is too long. Cut it in half and focus on the financial impact."

"I like this but it's missing the part about our return policy. Add a paragraph about that after the second section."

"Rewrite this but imagine the reader is a busy CEO who will skim it in 30 seconds."

Each of these takes 10 seconds to type. The people who think AI is useless tend to be the ones who treat it like a search engine: one query, one result, done. Treat it like a back-and-forth with a fast colleague and the results change completely.

## Real examples from actual business tasks

Let me walk through a few common scenarios to show the difference specificity makes.

### Drafting a job posting

*Weak prompt:* "Write a job posting for a marketing manager."

*Better prompt:* "Write a job posting for a marketing manager at a 40-person e-commerce company that sells outdoor gear. We need someone who can manage our social media, email campaigns, and two junior marketers. The role is hybrid, three days in-office in Austin. Salary range is $75-90K. Our tone is friendly and outdoorsy, not corporate. Keep it under 400 words and avoid phrases like 'rock star' or 'ninja.'"

### Summarizing customer feedback

*Weak prompt:* "Summarize this feedback."

*Better prompt:* "I'm pasting 50 customer feedback responses from our last quarter below. Group them into themes, tell me how many responses fall into each theme, and flag any recurring complaints that appeared more than three times. Format it as a brief I can share with my product team. Here are the responses: [paste data]"

### Analyzing a spreadsheet

*Weak prompt:* "Look at this data and tell me what's interesting."

*Better prompt:* "This spreadsheet contains our monthly sales data by product category for the last 12 months. I need you to: (1) identify which categories grew and which declined, (2) flag any months with unusual spikes or drops, and (3) suggest three questions I should be asking about this data that I might not have thought of. Format the response as a short memo to my team."

The pattern across all of these is the same. You're telling the AI who you are, what you need, how to format it, and what to focus on. None of this requires technical knowledge. It requires the same clarity you'd bring to briefing any competent person you've hired.

## The catch (because there's always a catch)

Better prompts get better results, but they don't eliminate the fundamental limitations of AI tools.

Even a perfectly prompted AI will still hallucinate sometimes. It'll still miss nuance. It'll still produce outputs that need human review before they go anywhere important. Getting good at prompting makes AI tools dramatically more useful. It doesn't make them infallible.

The other thing worth knowing: prompt skill improves with practice, but the first week or two can feel slow. Writing a detailed prompt might take three minutes when you could have just written the email yourself in five. The payoff comes after you've built up a set of go-to prompts for tasks you do repeatedly, and the prompting habit becomes second nature. Give yourself a month before you decide whether it's worth the effort.

## Save what works

Keep a note somewhere (a Google Doc, a Slack channel, a literal sticky note on your monitor) where you save prompts that gave you great results. Next time you need something similar, start from that prompt instead of from scratch.

We've seen teams build shared prompt libraries that save everyone real time. Your marketing person figures out a great prompt for writing social media posts, saves it, and suddenly the whole team is producing better content in less time.

**The best prompt you'll ever write is one you borrowed from a colleague who already figured it out.** Get your team sharing what works. That's worth more than any prompt engineering course.
