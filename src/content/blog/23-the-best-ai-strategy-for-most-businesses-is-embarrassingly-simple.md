---
title: "The best AI strategy for most businesses is embarrassingly simple"
subtitle: "No 90-page deck. No digital transformation. Just one painful process and two weeks."
date: 2025-11-21
author: g
category: honest-take
tags: ["ai strategy", "honest take"]
excerpt: "The best AI strategy for most businesses fits on a napkin. Pick one painful process, test a tool for two weeks, measure results, decide."
featured_image: "/images/blog/23-the-best-ai-strategy-for-most-businesses-is-embarrassingly-simple-art.png"
featured_image_alt: "A napkin with a simple four-step plan written on it next to a coffee cup"
featured: false
draft: false
---

There's an entire industry built around making AI strategy sound complicated. Consultancies sell six-figure "AI readiness assessments." Conference speakers fill 45-minute slots explaining why you need a "comprehensive AI roadmap" before you touch a single tool. Vendors want you to believe that AI adoption requires a digital transformation, an organizational restructuring, and probably a vision quest.

I get why this narrative exists. Complicated problems justify expensive solutions. And there are businesses where AI strategy genuinely is complex: regulated industries, companies with massive legacy systems, organizations where a wrong AI output could hurt someone. Those businesses need careful planning.

But most businesses reading this aren't those businesses. Most are companies with 20 to 150 people, trying to figure out whether AI can help them get more done without burning money on the wrong tools or confusing their teams.

For those businesses, the best AI strategy is so simple it's almost embarrassing.

## The strategy that fits on a napkin

Pick one painful, repetitive process. Try an AI tool on it for two weeks. Measure whether it helped. Then decide whether to expand.

That's it. That's the strategy.

No 90-page deck. No steering committee. No "digital transformation roadmap." No vendor evaluation matrix with weighted scoring across fourteen dimensions. Just: find a problem, try a solution, see if it works.

We recommend this approach constantly because we keep seeing it outperform the alternative. Businesses that start with a massive strategy spend months planning, thousands of dollars on consulting (yes, even ours), and often end up paralyzed by the number of options. Businesses that start with one experiment have results in two weeks and actual data to guide their next move.

## How to pick the right first problem

Not every process is a good first candidate. You want something repetitive, the kind of task someone on your team does every day or every week and would be thrilled to do less of. Summarizing meeting notes. Drafting initial responses to customer inquiries. Pulling data from one system into another. Generating weekly reports that follow the same structure every time.

Keep the stakes low. Your first AI experiment is not the place to automate financial compliance or medical intake forms. Pick something where a mistake is inconvenient, not catastrophic. If the AI drafts a mediocre internal email, someone edits it and moves on. If it miscalculates a regulatory filing, that's a very different situation.

**You also need a clear before-and-after**, and this is the part most people skip. How long does the task take now? How often does it happen? What does "good" look like? Without that baseline, you can't tell whether the AI actually helped or just felt novel for a few days.

One more thing: ideally, someone on the team is already frustrated with the process. Enthusiasm from at least one person who will actually use the tool makes the difference between a two-week experiment that produces real learning and one where nobody logs in after day three.

## The two-week trial

Two weeks is long enough to get past the novelty phase and short enough that nobody resents the disruption.

Here's what the two weeks should look like in practice.

Before you start, document how the process currently works. How long does it take? Who does it? How often? What's the output quality? Write this down. It doesn't need to be a formal document. A paragraph in a shared doc is fine.

Week one is learning. The person doing the trial is figuring out how the tool works, what prompts or settings produce the best results, and where the tool struggles. Expect frustration in week one. This is normal. If you've never used an AI tool on real work before, there's a learning curve. The tool won't read your mind. You'll need to experiment with how you ask it to do things.

Week two is execution. The person has their bearings now. They're using the tool on their actual work and comparing it to the old way. Is it faster? Is the quality acceptable? Are they spending more time fixing the AI's output than they'd spend doing it manually?

At the end of two weeks, you have something most AI strategies never produce: real data from your actual business.

## Measuring what matters

The measurements don't need to be sophisticated. You're looking for clear signals, not statistically significant results.

Time saved is the obvious one. If the task used to take four hours a week and now takes two, that's real. But be careful here. **Measure the total time, including the time spent reviewing, editing, and fixing the AI's output.** A tool that drafts something in ten seconds but requires thirty minutes of editing hasn't saved you much.

Quality is harder to measure but worth paying attention to. Is the output from the AI good enough to use with minor edits? Or does it need substantial rework every time? "Good enough with minor edits" is the bar for most business processes. You're not looking for perfection.

Adoption tells you something about sustainability. Did the person using the tool keep using it through week two, or did they quietly revert to the old way? If they reverted, find out why. The reason is usually more instructive than any metric.

And satisfaction matters. Ask the person who ran the trial: would you keep using this? Do you want to? Their gut feeling, after two weeks of real use, is worth more than any analyst's framework.

## The decision after the trial

After two weeks, you'll be in one of a few places.

Maybe it worked and the team wants to keep going. Great. Expand to more people doing the same task, or pick the next process to test. You've validated an AI use case with real evidence from your own business.

More commonly, it sort of worked but needs adjustment. The tool helped, but the prompts need refining, or the workflow needs tweaking, or the team needs more training. Give it another two weeks before writing it off. Most AI tools don't deliver their full value until the user has figured out how to work with them.

Sometimes it doesn't work for this particular use case. That's fine. You've spent two weeks and maybe a few hundred dollars. You now know, from evidence, that this tool doesn't solve this problem well enough. That's not failure. That's a faster and cheaper answer than any strategy project would give you.

The tricky outcome, and it happens more than you'd expect, is when the tool works but the team won't use it. The AI produces good output, but the person doesn't trust it, or doesn't want to change how they work, or found the interface annoying. That's a people problem, not a technology problem. It usually means you need better onboarding or a different approach to rolling it out.

## Why simple strategies outperform complex ones

I think about this a lot, because our business model could benefit from selling complexity. We could pitch twelve-week AI strategy engagements and charge accordingly. We don't, because we've watched complex strategies fail too many times.

The most obvious reason is speed. By the time a six-month AI strategy is "complete," the tool landscape has shifted, the team's priorities have changed, and the momentum is gone. The AI tools available today are measurably better than the ones from six months ago. Planning slowly in a fast-moving space is its own risk.

There's also the paralysis problem. When you're evaluating forty potential AI use cases across eight departments with a scoring matrix, everything starts looking the same. Nobody can decide what to do first, so nothing happens. We've seen this with multiple businesses. The strategy becomes the project, and the actual AI work never starts.

Complex strategies also tend to be disconnected from reality. A consultant can spend weeks interviewing your team and analyzing workflows, but they'll never understand the texture of your daily work the way your people do. The person who does the task every day knows what's painful and what a good tool would need to do. Trust that knowledge over a framework.

But here's what really gets me: **complex strategies delay learning.** Every day you spend planning instead of experimenting is a day you're not building the practical AI skills your team needs. The only way to learn how AI tools work for your business is to try them on your business. No document can substitute for that.

## The catch (because there's always a catch)

The napkin strategy has limits. It works best for businesses with relatively straightforward operations who are getting started with AI. Regulated industries need governance before they experiment. Businesses handling sensitive customer data need to know where that data goes before piping it into any AI tool. And if your technical infrastructure is genuinely outdated, you might need some foundational work before AI tools can even plug in.

The other limitation is scope. The "pick one problem, test for two weeks" approach gets you your first win, but scaling from one process to ten requires more coordination. At some point, you do need someone thinking about how these tools work together, who has access to what, and what the cumulative costs look like.

But that planning is much easier and much more grounded when you've already run a few experiments and have real data about what works in your business.

## Start this week

The businesses that get the most from AI aren't the ones with the best strategies. They're the ones that started trying things earliest and learned the fastest.

Pick the most tedious, repetitive task your team does. Ask the person who does it what tool they'd want to try. Sign up for a two-week trial. Measure the results.

That's not a dumbed-down version of an AI strategy. That is the strategy. Everything else is just the next iteration.
